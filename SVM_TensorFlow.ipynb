{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM TensorFlow.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNt7Yt9dyEq4uSjwEnY+2Bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mckang6113875/AP_Test_OPEN/blob/main/SVM_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tH0_K0SeaXQb"
      },
      "outputs": [],
      "source": [
        "#6113875"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_graphics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuneAC57bRvh",
        "outputId": "8c7bc104-8fe0-487f-d980-4f8726fe2a87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_graphics in /usr/local/lib/python3.7/dist-packages (2021.12.3)\n",
            "Requirement already satisfied: tqdm>=4.45.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (4.64.0)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (3.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (2.6.3)\n",
            "Requirement already satisfied: tensorflow-addons>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (0.16.1)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (2.8.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (1.4.1)\n",
            "Requirement already satisfied: psutil>=5.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (5.9.0)\n",
            "Requirement already satisfied: trimesh>=2.37.22 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (3.12.0)\n",
            "Requirement already satisfied: OpenEXR>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (1.3.8)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-datasets>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (4.0.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_graphics) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.6.1->tensorflow_graphics) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->tensorflow_graphics) (1.5.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.5->tensorflow_graphics) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.5->tensorflow_graphics) (4.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.25.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.14.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (14.0.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.44.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->tensorflow_graphics) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->tensorflow_graphics) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->tensorflow_graphics) (3.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.10.0->tensorflow_graphics) (2.7.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.16.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (5.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (1.7.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (21.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=2.0.0->tensorflow_graphics) (0.3.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets>=2.0.0->tensorflow_graphics) (1.56.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The TensorFlow Authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Quadratic radial lens distortion and un-distortion functions.\n",
        "\n",
        "Given a vector in homogeneous coordinates, `(x/z, y/z, 1)`, we define\n",
        "`r^2 = (x/z)^2 + (y/z)^2`. We use the simplest form of distortion function,\n",
        "`f(r) = 1 + k * r^2`. The distorted vector is given by\n",
        "`(f(r) * x/z, f(r) * y/z, 1)`.\n",
        "\n",
        "To apply the undistortion, we need the inverse of f(r), g = f^{-1}. In this\n",
        "library we use the approximate formula for the undistortion function given here\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4934233/, and refine the solution\n",
        "using Newton-Raphson iterations (https://en.wikipedia.org/wiki/Newtons_method).\n",
        "\n",
        "Restricting the distortion function to quadratic form allows to easily detect\n",
        "the cases where `r` goes beyond the monotonically-increasing range of `f` (which\n",
        "we refer to as overflow).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from typing import Tuple\n",
        "from six.moves import range\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_graphics.util import asserts\n",
        "from tensorflow_graphics.util import export_api\n",
        "from tensorflow_graphics.util import shape\n",
        "from tensorflow_graphics.util import type_alias\n",
        "\n",
        "\n",
        "def distortion_factor(\n",
        "    squared_radius: type_alias.TensorLike,\n",
        "    distortion_coefficient: type_alias.TensorLike,\n",
        "    name: str = \"quadratic_radial_distortion_distortion_factor\"\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Calculates a quadratic distortion factor given squared radii.\n",
        "\n",
        "  Given a vector describing a location in camera space in homogeneous\n",
        "  coordinates, `(x/z, y/z, 1)`, squared_radius is `r^2 = (x/z)^2 + (y/z)^2`.\n",
        "  distortion_factor multiplies `x/z` and `y/z` to obtain the distorted\n",
        "  coordinates. In this function, `distortion_factor` is given by\n",
        "  `1.0 + distortion_coefficient * squared_radius`.\n",
        "\n",
        "  Note:\n",
        "    In the following, A1 to An are optional batch dimensions, which must be\n",
        "    broadcast compatible.\n",
        "\n",
        "  Args:\n",
        "    squared_radius: A tensor of shape `[A1, ..., An, H, W]`, containing the\n",
        "      radii of the image pixels computed as `(x/z)^2 + (y/z)^2`. We use squared\n",
        "      radius rather than the radius itself to avoid an unnecessary `sqrt`, which\n",
        "      may introduce gradient singularities. The non-negativity of squared radius\n",
        "      is only enforced in debug mode.\n",
        "    distortion_coefficient: A `scalar` or a tensor of shape `[A1, ..., An]`,\n",
        "      which contains the distortion coefficients of each image.\n",
        "    name: A name for this op. Defaults to\n",
        "      \"quadratic_radial_distortion_distortion_factor\".\n",
        "\n",
        "  Returns:\n",
        "    distortion_factor: A tensor of shape `[A1, ..., An, H, W]`, the correction\n",
        "      factor that should multiply the projective coordinates `(x/z)` and `(y/z)`\n",
        "      to apply the distortion.\n",
        "    overflow_mask: A boolean tensor of shape `[A1, ..., An, H, W]`, `True` where\n",
        "      `squared_radius` is beyond the range where the distortion function is\n",
        "      monotonically increasing. Wherever `overflow_mask` is True,\n",
        "      `distortion_factor`'s value is meaningless.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name,):\n",
        "    squared_radius = tf.convert_to_tensor(value=squared_radius)\n",
        "    distortion_coefficient = tf.convert_to_tensor(value=distortion_coefficient)\n",
        "\n",
        "    if distortion_coefficient.shape.ndims == 0:\n",
        "      distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=0)\n",
        "    shape.check_static(\n",
        "        tensor=squared_radius,\n",
        "        tensor_name=\"squared_radius\",\n",
        "        has_rank_greater_than=1)\n",
        "    shape.compare_batch_dimensions(\n",
        "        tensors=(squared_radius, distortion_coefficient),\n",
        "        tensor_names=(\"squared_radius\", \"distortion_coefficient\"),\n",
        "        last_axes=(-3, -1),\n",
        "        broadcast_compatible=True)\n",
        "    squared_radius = asserts.assert_all_above(\n",
        "        squared_radius, 0.0, open_bound=False)\n",
        "    distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=-1)\n",
        "    distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=-1)\n",
        "    distortion_coefficient_times_squared_radius = (\n",
        "        distortion_coefficient * squared_radius)\n",
        "    distortion_factor_ = 1.0 + distortion_coefficient_times_squared_radius\n",
        "    # This condition needs to hold for the distortion to be monotomnically\n",
        "    # increasing, as can be derived by differentiating it.\n",
        "    overflow_mask = tf.less(\n",
        "        1.0 + 3.0 * distortion_coefficient_times_squared_radius, 0.0)\n",
        "    return distortion_factor_, overflow_mask\n",
        "\n",
        "\n",
        "def undistortion_factor(\n",
        "    distorted_squared_radius: type_alias.TensorLike,\n",
        "    distortion_coefficient: type_alias.TensorLike,\n",
        "    num_iterations: int = 5,\n",
        "    name: str = \"quadratic_radial_distortion_undistortion_factor\"\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Calculates the inverse quadratic distortion function given squared radii.\n",
        "\n",
        "  Given a vector describing a location in camera space in homogeneous\n",
        "  coordinates `(x/z, y/z, 1)`, after distortion has been applied, these become\n",
        "  `(x'/z, y'/z, 1)`. `distorted_squared_radius` is `(x'/z)^2 + (y'/z)^2`.\n",
        "  `undistortion_factor` multiplies `x'/z` and `y'/z` to obtain the undistorted\n",
        "  projective coordinates `x/z` and `y/z`.\n",
        "  The undustortion factor in this function is derived from a quadratic.\n",
        "  distortion function, where the distortion factor equals\n",
        "  `1.0 + distortion_coefficient * squared_radius`.\n",
        "\n",
        "  Note:\n",
        "    In the following, A1 to An are optional batch dimensions, which must be\n",
        "    broadcast compatible.\n",
        "\n",
        "  Args:\n",
        "    distorted_squared_radius: A tensor of shape `[A1, ..., An, H, W]` containing\n",
        "      the value of  projective coordinates `(x/z)^2 + (y/z)^2`. For each pixel\n",
        "      it contains the squared distance of that pixel to the center of the image\n",
        "      plane. We use `distorted_squared_radius` rather than the distorted radius\n",
        "      itself to avoid an unnecessary `sqrt`, which may introduce gradient\n",
        "      singularities. The non-negativity of `distorted_squared_radius` is only\n",
        "      enforced in debug mode.\n",
        "    distortion_coefficient: A `scalar` or a tensor of shape `[A1, ..., An]`,\n",
        "      which contains the distortion coefficients of each image.\n",
        "    num_iterations: Number of Newton-Raphson iterations to calculate the inverse\n",
        "      distortion function. Defaults to 5, which is on the high-accuracy side.\n",
        "    name: A name for this op. Defaults to\n",
        "      \"quadratic_radial_distortion_undistortion_factor\".\n",
        "\n",
        "  Returns:\n",
        "    undistortion: A tensor of shape `[A1, ..., An, H, W]` containing the\n",
        "      correction factor that should multiply the distorted projective\n",
        "      coordinates `(x'/z)` and `(y'/z)` to obtain the undistorted ones.\n",
        "    overflow_mask: A `bool` tensor of shape `[A1, ..., An, H, W]`, `True` where\n",
        "      `distorted_squared_radius` is beyond the range where the distortion\n",
        "      function is monotonically increasing. Wherever `overflow_mask` is `True`,\n",
        "      `undistortion_factor`'s value is meaningless.\n",
        "\n",
        "  \"\"\"\n",
        "  with tf.name_scope(name):\n",
        "    distorted_squared_radius = tf.convert_to_tensor(\n",
        "        value=distorted_squared_radius)\n",
        "    distortion_coefficient = tf.convert_to_tensor(value=distortion_coefficient)\n",
        "\n",
        "    if distortion_coefficient.shape.ndims == 0:\n",
        "      distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=0)\n",
        "    shape.check_static(\n",
        "        tensor=distorted_squared_radius,\n",
        "        tensor_name=\"distorted_squared_radius\",\n",
        "        has_rank_greater_than=1)\n",
        "    shape.compare_batch_dimensions(\n",
        "        tensors=(distorted_squared_radius, distortion_coefficient),\n",
        "        tensor_names=(\"distorted_squared_radius\", \"distortion_coefficient\"),\n",
        "        last_axes=(-3, -1),\n",
        "        broadcast_compatible=True)\n",
        "    distorted_squared_radius = asserts.assert_all_above(\n",
        "        distorted_squared_radius, 0.0, open_bound=False)\n",
        "    distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=-1)\n",
        "    distortion_coefficient = tf.expand_dims(distortion_coefficient, axis=-1)\n",
        "    # For a distortion function of r' = (1 + ar^2)r, with a negative a, the\n",
        "    # maximum r until which r'(r) is monotonically increasing is r^2 = -1/(3a).\n",
        "    # At that value, r'^2 = -4 / (27a). Therefore the overflow condition for r'\n",
        "    # is ar'^2 +(4/27.0) < 0. For a positive a it never holds, as it should,\n",
        "    # because then r' is monotonic in r everywhere and thus never overflows.\n",
        "    distortion_coefficient_times_distorted_squared_radius = (\n",
        "        distortion_coefficient * distorted_squared_radius)\n",
        "    overflow_mask = tf.less(\n",
        "        4.0 / 27.0 + distortion_coefficient_times_distorted_squared_radius, 0.0)\n",
        "\n",
        "    # Newton-raphson iterations. The expression below is obtained from\n",
        "    # algebrically simplifying the Newton-Raphson formula\n",
        "    # (https://en.wikipedia.org/wiki/Newtons_method).\n",
        "    # We initialize with the approximate formula for the undistortion function\n",
        "    # given here https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4934233/.\n",
        "    undistortion = (1.0 - distortion_coefficient_times_distorted_squared_radius)\n",
        "    for _ in range(num_iterations):\n",
        "      two_thirds_undistortion = 2.0 * undistortion / 3.0\n",
        "      undistortion = (1.0 - two_thirds_undistortion) / (\n",
        "          1.0 + 3.0 * distortion_coefficient_times_distorted_squared_radius *\n",
        "          undistortion * undistortion) + two_thirds_undistortion\n",
        "    return undistortion, overflow_mask\n",
        "\n",
        "\n",
        "# API contains all public functions and classes.\n",
        "__all__ = export_api.get_functions_and_classes()"
      ],
      "metadata": {
        "id": "cLloNp_GbHcy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The TensorFlow Authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"tensorflow_graphics module.\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# pylint: disable=g-statement-before-imports,g-import-not-at-top\n",
        "try:\n",
        "  import tensorflow as tf\n",
        "except ImportError:\n",
        "  print(\"Warning: TensorFlow is not installed when you install TensorFlow\"\n",
        "        \" Graphics. To use TensorFlow Graphics, please install TensorFlow, by\"\n",
        "        \" following instructions at https://tensorflow.org/install or by using\"\n",
        "        \" pip install tensorflow_graphics[tf] or\"\n",
        "        \" pip install tensorflow_graphics[tf-gpu].\")\n",
        "# pylint: enable=g-statement-before-imports,g-import-not-at-top\n",
        "\n",
        "# pylint: disable=g-statement-before-imports,g-import-not-at-top,ungrouped-imports\n",
        "from tensorflow_graphics.util.doc import _import_tfg_docs\n",
        "if _import_tfg_docs():\n",
        "  from tensorflow_graphics import datasets\n",
        "  from tensorflow_graphics import geometry\n",
        "  from tensorflow_graphics import image\n",
        "  from tensorflow_graphics import math\n",
        "  from tensorflow_graphics import nn\n",
        "  from tensorflow_graphics import notebooks\n",
        "  from tensorflow_graphics import projects\n",
        "  from tensorflow_graphics import rendering\n",
        "  from tensorflow_graphics import util\n",
        "\n",
        "  # submodules of tensorflow_graphics\n",
        "  __all__ = util.export_api.get_modules()\n",
        "\n",
        "  # Remove modules projects, notebooks, util and version from API.\n",
        "  __all__.remove(\"projects\")\n",
        "  __all__.remove(\"notebooks\")\n",
        "  __all__.remove(\"util\")\n",
        "# pylint: enable=g-statement-before-imports,g-import-not-at-top\n",
        "\n",
        "__version__ = \"HEAD\""
      ],
      "metadata": {
        "id": "3E2b3jx9cNwK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distort(images, d, name='distort'):\n",
        "        def _repeat(x, n_repeats):\n",
        "            with tf.variable_scope('_repeat'):\n",
        "                rep = tf.transpose(\n",
        "                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])\n",
        "                rep = tf.cast(rep, 'int32')\n",
        "                x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n",
        "                return tf.reshape(x, [-1])\n",
        "\n",
        "        def _interpolate(im, x, y, out_size):\n",
        "            with tf.variable_scope('_interpolate'):\n",
        "                # constants\n",
        "                num_batch = tf.shape(im)[0]\n",
        "                height = tf.shape(im)[1]\n",
        "                width = tf.shape(im)[2]\n",
        "                channels = tf.shape(im)[3]\n",
        "\n",
        "                x = tf.cast(x, 'float32')\n",
        "                y = tf.cast(y, 'float32')\n",
        "                height_f = tf.cast(height, 'float32')\n",
        "                width_f = tf.cast(width, 'float32')\n",
        "                out_height = out_size[0]\n",
        "                out_width = out_size[1]\n",
        "                zero = tf.zeros([], dtype='int32')\n",
        "                max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')\n",
        "                max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')\n",
        "\n",
        "                # scale indices from [-1, 1] to [0, width/height]\n",
        "                x = (x + 1.0)*(width_f) / 2.0\n",
        "                y = (y + 1.0)*(height_f) / 2.0\n",
        "\n",
        "                # do sampling\n",
        "                x0 = tf.cast(tf.floor(x), 'int32')\n",
        "                x1 = x0 + 1\n",
        "                y0 = tf.cast(tf.floor(y), 'int32')\n",
        "                y1 = y0 + 1\n",
        "\n",
        "                x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "                x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "                y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "                y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "                dim2 = width\n",
        "                dim1 = width*height\n",
        "                base = _repeat(tf.range(num_batch)*dim1, out_height*out_width)\n",
        "                base_y0 = base + y0*dim2\n",
        "                base_y1 = base + y1*dim2\n",
        "                idx_a = base_y0 + x0\n",
        "                idx_b = base_y1 + x0\n",
        "                idx_c = base_y0 + x1\n",
        "                idx_d = base_y1 + x1\n",
        "\n",
        "                # use indices to lookup pixels in the flat image and restore\n",
        "                # channels dim\n",
        "                im_flat = tf.reshape(im, tf.stack([-1, channels]))\n",
        "                im_flat = tf.cast(im_flat, 'float32')\n",
        "                Ia = tf.gather(im_flat, idx_a)\n",
        "                Ib = tf.gather(im_flat, idx_b)\n",
        "                Ic = tf.gather(im_flat, idx_c)\n",
        "                Id = tf.gather(im_flat, idx_d)\n",
        "\n",
        "                # and finally calculate interpolated values\n",
        "                x0_f = tf.cast(x0, 'float32')\n",
        "                x1_f = tf.cast(x1, 'float32')\n",
        "                y0_f = tf.cast(y0, 'float32')\n",
        "                y1_f = tf.cast(y1, 'float32')\n",
        "                wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)\n",
        "                wb = tf.expand_dims(((x1_f-x) * (y-y0_f)), 1)\n",
        "                wc = tf.expand_dims(((x-x0_f) * (y1_f-y)), 1)\n",
        "                wd = tf.expand_dims(((x-x0_f) * (y-y0_f)), 1)\n",
        "                output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
        "                return output\n",
        "\n",
        "            def _transform(images, d, out_size):\n",
        "                with tf.variable_scope('_transform'): \n",
        "\n",
        "                    shape = tf.shape(images)\n",
        "                    num_batch = tf.shape(images)[0]\n",
        "                    num_channels = images.get_shape()[3]\n",
        "\n",
        "                    out_width = out_size[1]\n",
        "                    out_height = out_size[0]\n",
        "                    cx = fx = fy = tf.to_float(out_width) / 2\n",
        "                    cy = tf.to_float(out_height) / 2\n",
        "                    x = tf.linspace(-1., 1., out_width)\n",
        "                    y = tf.linspace(-1., 1., out_height)\n",
        "                    x, y = tf.meshgrid(x, y)\n",
        "                    x = tf.tile(tf.reshape(x, [1, -1, 1]), [num_batch,1,1])\n",
        "                    y = tf.tile(tf.reshape(y, [1, -1, 1]), [num_batch,1,1])\n",
        "\n",
        "                    a = x \n",
        "                    b = y \n",
        "\n",
        "                    r2 = tf.square(a) + tf.square(b)\n",
        "\n",
        "                    r = tf.sqrt(r2)\n",
        "                    r = tf.Print(r, [tf.reduce_min(r), tf.reduce_max(r)], \"R min/max: \")\n",
        "                    theta = tf.atan(r)\n",
        "                    theta_d = theta*(1.0 + tf.reduce_sum(tf.reshape(d,\n",
        "                          [1,1,4]) * tf.concat([tf.square(theta),\n",
        "                          tf.pow(theta, 4), tf.pow(theta, 6), tf.pow(theta, \n",
        "                          8)], axis=-1),\n",
        "                          axis=-1, keepdims=True))\n",
        "                    tdr = theta_d / r\n",
        "                    xd = a * tdr\n",
        "                    yd = b * tdr\n",
        "\n",
        "\n",
        "                    xd = tf.reshape(xd, [-1])\n",
        "                    yd = tf.reshape(yd, [-1])\n",
        "\n",
        "                    input_transformed = _interpolate(\n",
        "                            images, xd, yd,\n",
        "                            out_size)\n",
        "                    output = tf.reshape(input_transformed, \n",
        "                             tf.stack([num_batch, out_height, out_width, \n",
        "                             num_channels]))\n",
        "                    return output\n",
        "            with tf.variable_scope(name):\n",
        "                output = _transform(images, d, tf.shape(images)[1:3])\n",
        "                return output"
      ],
      "metadata": {
        "id": "yj9rxq63c3lp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The TensorFlow Authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#    https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Camera feature.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_datasets import features\n",
        "\n",
        "from tensorflow_graphics.datasets.features import pose_feature\n",
        "\n",
        "\n",
        "class Camera(features.FeaturesDict):\n",
        "  \"\"\"`FeatureConnector` for camera calibration (extrinsic and intrinsic).\n",
        "\n",
        "  During `_generate_examples`, the feature connector accepts as input:\n",
        "\n",
        "    * `parameter_dict:` A dictionary containing the extrinsic and instrinsic\n",
        "    parameters of the camera as:\n",
        "      - 'pose': Dictionary containing\n",
        "          * Either 3x3 rotation matrix and translation vector:\n",
        "            {\n",
        "              'R': A `float32` tensor with shape `[3, 3]` denoting the\n",
        "                   3D rotation matrix.\n",
        "              't': A `float32` tensor with shape `[3,]` denoting the\n",
        "                   translation vector.\n",
        "            }\n",
        "      OR\n",
        "         * look_at, position and up-vector:\n",
        "            {\n",
        "              'look_at': float32 vector of shape (3,).\n",
        "              'position': float32 vector of shape (3,).\n",
        "              'up': float32 vector of shape (3,).\n",
        "            }\n",
        "      - 'f': focal length of the camera in pixel (either single float32 value\n",
        "      or tuple of float32 as (f_x, f_y).\n",
        "      - 'optical_center': Optical center of the camera\n",
        "      in pixel coordinates as tuple (c_x, c_y)\n",
        "      Optional parameters:\n",
        "      - 'skew': float32 denoting the skew of the camera axes.\n",
        "      - 'aspect_ratio': float32 denoting the aspect_ratio,\n",
        "      if single fixed focal length is provided.\n",
        "\n",
        "\n",
        "  Output:\n",
        "    A dictionary containing:\n",
        "\n",
        "    * 'pose': A `tensorflow_graphics.datasets.features.Pose` FeatureConnector\n",
        "    representing the 3D pose of the camera.\n",
        "    * 'intrinsics': A `float32` tensor with shape `[3,3]` denoting the intrinsic\n",
        "    matrix.\n",
        "\n",
        "  Example:\n",
        "    Default values for skew (s) and aspect_ratio(a) are 0 and 1, respectively.\n",
        "\n",
        "    Full calibration matrix:\n",
        "      K = [[ f_x,   s, c_x ],\n",
        "           [   0, f_y, c_y ],\n",
        "           [   0,   0,  1  ]]\n",
        "\n",
        "    With same focal length:\n",
        "      K = [[ f,  s, c_x ],\n",
        "           [ 0, af, c_y ],\n",
        "           [ 0,  0,  1  ]]\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Camera, self).__init__({\n",
        "        'pose': pose_feature.Pose(),\n",
        "        'intrinsics': features.Tensor(shape=(3, 3), dtype=tf.float32),\n",
        "    })\n",
        "\n",
        "  def encode_example(self, example_dict):\n",
        "    \"\"\"Convert the given parameters into a dict convertible to tf example.\"\"\"\n",
        "    REQUIRED_KEYS = ['pose', 'f', 'optical_center']  # pylint: disable=invalid-name\n",
        "    if not all(key in example_dict for key in REQUIRED_KEYS):\n",
        "      raise ValueError(f'Missing keys in provided dictionary! '\n",
        "                       f'Expected {REQUIRED_KEYS}, '\n",
        "                       f'but {example_dict.keys()} were given.')\n",
        "\n",
        "    if not isinstance(example_dict['pose'], dict):\n",
        "      raise ValueError('Pose needs to be a dictionary containing either '\n",
        "                       'rotation and translation or look at, '\n",
        "                       'up vector and position.')\n",
        "    features_dict = {}\n",
        "    pose_dict = example_dict['pose']\n",
        "    if all(key in pose_dict for key in ['R', 't']):\n",
        "      features_dict['pose'] = {\n",
        "          'R': pose_dict['R'],\n",
        "          't': pose_dict['t']\n",
        "      }\n",
        "    elif all(key in pose_dict for key in ['look_at', 'position', 'up']):\n",
        "      rotation = self._create_rotation_from_look_at(pose_dict['look_at'],\n",
        "                                                    pose_dict['position'],\n",
        "                                                    pose_dict['up'])\n",
        "      translation = (-rotation) @ pose_dict['position']\n",
        "\n",
        "      features_dict['pose'] = {\n",
        "          'R': rotation,\n",
        "          't': translation\n",
        "      }\n",
        "    else:\n",
        "      raise ValueError('Wrong keys for pose feature provided!')\n",
        "\n",
        "    aspect_ratio = 1\n",
        "    skew = 0\n",
        "    if 'aspect_ratio' in example_dict.keys():\n",
        "      if not isinstance(example_dict['f'], float):\n",
        "        raise ValueError('If aspect ratio is provided, '\n",
        "                         'f needs to be a single float.')\n",
        "      aspect_ratio = example_dict['aspect_ratio']\n",
        "\n",
        "    if 'skew' in example_dict.keys():\n",
        "      skew = example_dict['skew']\n",
        "\n",
        "    features_dict['intrinsics'] = self._create_calibration_matrix(\n",
        "        example_dict['f'],\n",
        "        example_dict['optical_center'],\n",
        "        aspect_ratio,\n",
        "        skew\n",
        "    )\n",
        "\n",
        "    return super(Camera, self).encode_example(features_dict)\n",
        "\n",
        "  def _create_rotation_from_look_at(self, look_at, position, up):\n",
        "    \"\"\"Creates rotation matrix according to OpenGL gluLookAt convention.\n",
        "\n",
        "    Args:\n",
        "      look_at: A float32 3D vector of look_at direction.\n",
        "      position: A float32 3D vector of camera position.\n",
        "      up: A float32 3D up direction vector.\n",
        "\n",
        "    Returns:\n",
        "      A 3x3 float32 rotation matrix.\n",
        "\n",
        "    (https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/xhtml/gluLookAt.xml)\n",
        "    \"\"\"\n",
        "    dir_vec = look_at - position\n",
        "    dir_vec /= np.linalg.norm(dir_vec)\n",
        "    side_vec = np.cross(dir_vec, up)\n",
        "    side_vec /= np.linalg.norm(side_vec)\n",
        "    up_vec = np.cross(side_vec, dir_vec)\n",
        "    matrix = np.array([side_vec, up_vec, -dir_vec])\n",
        "\n",
        "    return matrix\n",
        "\n",
        "  def _create_calibration_matrix(self, f, optical_center, aspect_ratio=1,\n",
        "                                 skew=0):\n",
        "    \"\"\"Constructs the 3x3 calibration matrix K.\n",
        "\n",
        "    Args:\n",
        "      f: Focal length of the camera. Either single float.32 value or tuple of\n",
        "        float32 when different focal lengths for each axis are provided (fx, fy)\n",
        "      optical_center: Tuple (c_x, c_y) containing the optical center\n",
        "        of the camera in pixel coordinates.\n",
        "      aspect_ratio: Optional parameter, if fixed focal length for both\n",
        "        dimensions is used. Defaults to 1.\n",
        "      skew: Optional parameter denoting the skew between the camera axes.\n",
        "\n",
        "    Returns:\n",
        "      float32 Tensor of shape [3,3] containing the upper triangular\n",
        "      calibration matrix K.\n",
        "    \"\"\"\n",
        "    if not isinstance(optical_center, tuple):\n",
        "      raise ValueError('Optical center of camera needs '\n",
        "                       'to be a tuple of (c_x, c_y).')\n",
        "\n",
        "    if isinstance(f, tuple):\n",
        "      f_x, f_y = f\n",
        "    else:\n",
        "      f_x = f\n",
        "      f_y = aspect_ratio * f\n",
        "\n",
        "    return np.asarray([[f_x, skew, optical_center[0]],\n",
        "                       [0, f_y, optical_center[1]],\n",
        "                       [0, 0, 1]\n",
        "                       ], dtype=np.float32)\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_content(cls, value) -> 'Camera':\n",
        "    return cls()\n",
        "\n",
        "  def to_json_content(self):\n",
        "    return {}"
      ],
      "metadata": {
        "id": "EyEdsE94duIh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/tensorflow/graphics"
      ],
      "metadata": {
        "id": "6Yiwh87sd7ox"
      }
    }
  ]
}